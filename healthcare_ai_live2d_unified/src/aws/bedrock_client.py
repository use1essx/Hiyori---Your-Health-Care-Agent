"""
AWS Bedrock Integration Client
=============================

Centralized Bedrock client with cost-optimized model selection and fallback strategies.
Provides agent-specific prompt templates and Traditional Chinese language processing.
"""

import json
import logging
from typing import Dict, Any, List, Optional, Tuple
from enum import Enum
from datetime import datetime
import time

# Import optimization system
try:
    from .lambda_optimizer import get_optimized_bedrock_client, lambda_optimizer
    OPTIMIZATION_AVAILABLE = True
except ImportError:
    # Fallback for when optimization is not available
    import boto3
    OPTIMIZATION_AVAILABLE = False

logger = logging.getLogger(__name__)


class ModelTier(Enum):
    """Model tiers for cost optimization."""
    FAST = "fast"
    BALANCED = "balanced"
    ADVANCED = "advanced"


class BedrockModelManager:
    """Manages Bedrock model selection and cost optimization."""
    
    def __init__(self):
        if OPTIMIZATION_AVAILABLE:
            self.client = get_optimized_bedrock_client()
        else:
            import boto3
            self.client = boto3.client('bedrock-runtime')
        
        # Cost-optimized model hierarchy
        self.models = {
            ModelTier.FAST: {
                'id': 'amazon.titan-text-lite-v1',
                'cost_per_1k_tokens': 0.0003,
                'max_tokens': 4000,
                'best_for': ['simple_queries', 'quick_responses']
            },
            ModelTier.BALANCED: {
                'id': 'anthropic.claude-3-haiku-20240307-v1:0',
                'cost_per_1k_tokens': 0.00025,
                'max_tokens': 200000,
                'best_for': ['healthcare_conversations', 'cultural_adaptation']
            },
            ModelTier.ADVANCED: {
                'id': 'anthropic.claude-3-sonnet-20240229-v1:0',
                'cost_per_1k_tokens': 0.003,
                'max_tokens': 200000,
                'best_for': ['complex_medical', 'crisis_intervention']
            }
        }
        
        # Usage tracking for cost monitoring
        self.usage_stats = {
            'total_requests': 0,
            'total_tokens': 0,
            'total_cost': 0.0,
            'model_usage': {tier.value: 0 for tier in ModelTier}
        }
    
    def select_optimal_model(self, agent_type: str, message_length: int, 
                           complexity_hints: List[str] = None) -> ModelTier:
        """Select optimal model based on agent type and message complexity."""
        complexity_hints = complexity_hints or []
        
        # Agent-specific model preferences
        agent_preferences = {
            'safety_guardian': ModelTier.BALANCED,  # Reliability over cost for emergencies
            'mental_health': ModelTier.BALANCED,    # Consistent responses for mental health
            'illness_monitor': ModelTier.BALANCED,  # Medical accuracy important
            'wellness_coach': ModelTier.FAST        # Motivational content can use faster model
        }
        
        # Start with agent preference
        preferred_tier = agent_preferences.get(agent_type, ModelTier.BALANCED)
        
        # Upgrade for complex scenarios
        if any(hint in complexity_hints for hint in ['crisis', 'emergency', 'complex_medical']):
            return ModelTier.ADVANCED
        
        # Upgrade for long messages
        if message_length > 500:
            if preferred_tier == ModelTier.FAST:
                return ModelTier.BALANCED
            elif preferred_tier == ModelTier.BALANCED:
                return ModelTier.ADVANCED
        
        return preferred_tier
    
    def get_model_config(self, tier: ModelTier) -> Dict[str, Any]:
        """Get model configuration for specified tier."""
        return self.models[tier]
    
    def track_usage(self, tier: ModelTier, tokens_used: int):
        """Track model usage for cost monitoring."""
        self.usage_stats['total_requests'] += 1
        self.usage_stats['total_tokens'] += tokens_used
        self.usage_stats['model_usage'][tier.value] += 1
        
        cost = (tokens_used / 1000) * self.models[tier]['cost_per_1k_tokens']
        self.usage_stats['total_cost'] += cost
    
    def get_usage_report(self) -> Dict[str, Any]:
        """Get current usage statistics."""
        return {
            **self.usage_stats,
            'timestamp': datetime.utcnow().isoformat(),
            'average_cost_per_request': (
                self.usage_stats['total_cost'] / max(1, self.usage_stats['total_requests'])
            )
        }


class BedrockClient:
    """Main Bedrock client with fallback strategies and prompt management."""
    
    def __init__(self):
        self.model_manager = BedrockModelManager()
        self.prompt_templates = BedrockPromptTemplates()
        
        # Fallback strategy configuration
        self.max_retries = 3
        self.retry_delay = 1.0
    
    async def generate_response(self, 
                              message: str,
                              agent_type: str,
                              context: Dict[str, Any],
                              preferred_tier: Optional[ModelTier] = None) -> Dict[str, Any]:
        """
        Generate AI response with automatic model selection and fallback.
        
        Args:
            message: User input message
            agent_type: Type of healthcare agent
            context: Conversation context including user profile
            preferred_tier: Optional preferred model tier
            
        Returns:
            Response dictionary with content and metadata
        """
        # Determine optimal model
        if preferred_tier is None:
            complexity_hints = self._analyze_complexity(message, context)
            preferred_tier = self.model_manager.select_optimal_model(
                agent_type, len(message), complexity_hints
            )
        
        # Get system prompt for agent
        system_prompt = self.prompt_templates.get_agent_prompt(agent_type, context)
        
        # Try models in fallback order
        fallback_order = self._get_fallback_order(preferred_tier)
        
        for attempt, tier in enumerate(fallback_order):
            try:
                response = await self._invoke_model(message, system_prompt, tier, context)
                
                # Track successful usage
                self.model_manager.track_usage(tier, response.get('tokens_used', 0))
                
                return {
                    **response,
                    'model_tier': tier.value,
                    'attempt': attempt + 1,
                    'fallback_used': attempt > 0
                }
                
            except Exception as e:
                logger.warning(f"Model {tier.value} failed (attempt {attempt + 1}): {e}")
                
                if attempt < len(fallback_order) - 1:
                    await self._wait_retry_delay(attempt)
                    continue
                else:
                    # All models failed, return fallback response
                    return self._get_fallback_response(agent_type, context)
    
    async def _invoke_model(self, message: str, system_prompt: str, 
                          tier: ModelTier, context: Dict[str, Any]) -> Dict[str, Any]:
        """Invoke specific Bedrock model."""
        model_config = self.model_manager.get_model_config(tier)
        model_id = model_config['id']
        
        # Prepare request based on model type
        if 'claude' in model_id:
            body = self._prepare_claude_request(message, system_prompt, model_config, context)
        else:  # Titan model
            body = self._prepare_titan_request(message, system_prompt, model_config, context)
        
        # Invoke model
        response = self.model_manager.client.invoke_model(
            modelId=model_id,
            body=json.dumps(body)
        )
        
        result = json.loads(response['body'].read())
        
        # Extract content based on model type
        if 'claude' in model_id:
            content = result['content'][0]['text']
            tokens_used = result.get('usage', {}).get('total_tokens', 0)
        else:  # Titan model
            content = result['results'][0]['outputText']
            tokens_used = len(content.split()) * 1.3  # Rough estimate for Titan
        
        return {
            'content': content,
            'model_used': model_id,
            'tokens_used': int(tokens_used),
            'confidence_score': self._calculate_confidence(content, tier),
            'processing_time': time.time()
        }
    
    def _prepare_claude_request(self, message: str, system_prompt: str, 
                              model_config: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare request for Claude models."""
        # Agent-specific temperature settings
        temperature_map = {
            'safety_guardian': 0.3,    # Low for consistent emergency responses
            'mental_health': 0.6,      # Moderate for empathetic but consistent responses
            'illness_monitor': 0.5,    # Balanced for medical accuracy
            'wellness_coach': 0.7      # Higher for motivational creativity
        }
        
        agent_type = context.get('agent_type', 'wellness_coach')
        temperature = temperature_map.get(agent_type, 0.6)
        
        return {
            'anthropic_version': 'bedrock-2023-05-31',
            'max_tokens': min(1200, model_config['max_tokens']),
            'system': system_prompt,
            'messages': [{'role': 'user', 'content': message}],
            'temperature': temperature,
            'top_p': 0.9
        }
    
    def _prepare_titan_request(self, message: str, system_prompt: str,
                             model_config: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """Prepare request for Titan models."""
        agent_type = context.get('agent_type', 'wellness_coach')
        temperature = 0.7 if agent_type == 'wellness_coach' else 0.5
        
        return {
            'inputText': f"System: {system_prompt}\n\nUser: {message}",
            'textGenerationConfig': {
                'maxTokenCount': min(1200, model_config['max_tokens']),
                'temperature': temperature,
                'topP': 0.9
            }
        }
    
    def _analyze_complexity(self, message: str, context: Dict[str, Any]) -> List[str]:
        """Analyze message complexity to determine appropriate model tier."""
        hints = []
        message_lower = message.lower()
        
        # Crisis indicators
        crisis_keywords = [
            'emergency', 'ç·Šæ€¥', 'crisis', 'å±æ©Ÿ', 'suicide', 'è‡ªæ®º',
            'chest pain', 'èƒ¸ç—›', 'can\'t breathe', 'å‘¼å¸å›°é›£'
        ]
        if any(keyword in message_lower for keyword in crisis_keywords):
            hints.append('crisis')
        
        # Medical complexity
        medical_keywords = [
            'diagnosis', 'è¨ºæ–·', 'medication', 'è—¥ç‰©', 'chronic', 'æ…¢æ€§',
            'symptoms', 'ç—‡ç‹€', 'treatment', 'æ²»ç™‚'
        ]
        if sum(1 for keyword in medical_keywords if keyword in message_lower) >= 2:
            hints.append('complex_medical')
        
        # Long or multi-part questions
        if len(message) > 300 or message.count('?') > 2:
            hints.append('complex_query')
        
        return hints
    
    def _get_fallback_order(self, preferred_tier: ModelTier) -> List[ModelTier]:
        """Get fallback order starting from preferred tier."""
        all_tiers = [ModelTier.ADVANCED, ModelTier.BALANCED, ModelTier.FAST]
        
        # Start with preferred tier, then others in descending order of capability
        fallback_order = [preferred_tier]
        for tier in all_tiers:
            if tier != preferred_tier:
                fallback_order.append(tier)
        
        return fallback_order
    
    async def _wait_retry_delay(self, attempt: int):
        """Wait with exponential backoff."""
        delay = self.retry_delay * (2 ** attempt)
        await asyncio.sleep(min(delay, 10.0))  # Cap at 10 seconds
    
    def _calculate_confidence(self, content: str, tier: ModelTier) -> float:
        """Calculate confidence score based on model tier and response quality."""
        base_confidence = {
            ModelTier.ADVANCED: 0.95,
            ModelTier.BALANCED: 0.85,
            ModelTier.FAST: 0.75
        }
        
        confidence = base_confidence[tier]
        
        # Adjust based on response quality indicators
        if len(content) < 50:  # Very short responses might be incomplete
            confidence -= 0.1
        elif len(content) > 500:  # Comprehensive responses
            confidence += 0.05
        
        return max(0.5, min(0.99, confidence))
    
    def _get_fallback_response(self, agent_type: str, context: Dict[str, Any]) -> Dict[str, Any]:
        """Get fallback response when all models fail."""
        language = context.get('language_preference', 'en')
        
        fallback_responses = {
            'safety_guardian': {
                'en': "ðŸš¨ EMERGENCY SYSTEM ERROR ðŸš¨\n\nIf this is a medical emergency, call 999 immediately.\nFor mental health crisis, call Samaritans 2896 0000.\nPlease seek immediate professional help.",
                'zh': "ðŸš¨ ç·Šæ€¥ç³»çµ±éŒ¯èª¤ ðŸš¨\n\nå¦‚æžœé€™æ˜¯é†«ç™‚ç·Šæ€¥æƒ…æ³ï¼Œè«‹ç«‹å³è‡´é›»999ã€‚\nå¿ƒç†å¥åº·å±æ©Ÿè«‹è‡´é›»æ’’ç‘ªåˆ©äºžæœƒ 2896 0000ã€‚\nè«‹ç«‹å³å°‹æ±‚å°ˆæ¥­å¹«åŠ©ã€‚"
            },
            'mental_health': {
                'en': "âœ¨ I'm here for you, but I'm having technical difficulties. Your feelings are important - please reach out to a trusted adult or call Samaritans 2896 0000 if you need support.",
                'zh': "âœ¨ æˆ‘åœ¨é€™è£¡æ”¯æŒä½ ï¼Œä½†ç¾åœ¨é‡åˆ°æŠ€è¡“å•é¡Œã€‚ä½ çš„æ„Ÿå—å¾ˆé‡è¦ - è«‹è¯ç¹«ä¿¡ä»»çš„æˆå¹´äººæˆ–è‡´é›»æ’’ç‘ªåˆ©äºžæœƒ 2896 0000ã€‚"
            },
            'illness_monitor': {
                'en': "ðŸ¥ I'm experiencing technical difficulties. For health concerns, please consult your doctor or call 999 for emergencies.",
                'zh': "ðŸ¥ æˆ‘é‡åˆ°æŠ€è¡“å•é¡Œã€‚å¥åº·å•é¡Œè«‹è«®è©¢é†«ç”Ÿï¼Œç·Šæ€¥æƒ…æ³è«‹è‡´é›»999ã€‚"
            },
            'wellness_coach': {
                'en': "ðŸ’ª I'm having technical difficulties, but remember: small steps toward better health make a big difference. Stay hydrated, get some movement, and prioritize rest!",
                'zh': "ðŸ’ª æˆ‘é‡åˆ°æŠ€è¡“å•é¡Œï¼Œä½†è¨˜ä½ï¼šé‚å‘æ›´å¥½å¥åº·çš„å°æ­¥é©Ÿæœƒå¸¶ä¾†å¤§æ”¹è®Šã€‚ä¿æŒæ°´åˆ†ï¼Œå¤šæ´»å‹•ï¼Œå„ªå…ˆä¼‘æ¯ï¼"
            }
        }
        
        response_text = fallback_responses.get(agent_type, fallback_responses['wellness_coach'])
        content = response_text.get(language, response_text['en'])
        
        return {
            'content': content,
            'model_used': 'fallback',
            'tokens_used': 0,
            'confidence_score': 0.8,  # High confidence for safety fallbacks
            'processing_time': time.time(),
            'model_tier': 'fallback',
            'attempt': 1,
            'fallback_used': True
        }


class BedrockPromptTemplates:
    """Manages agent-specific prompt templates."""
    
    def __init__(self):
        self.templates = self._load_prompt_templates()
    
    def get_agent_prompt(self, agent_type: str, context: Dict[str, Any]) -> str:
        """Get system prompt for specific agent with context adaptation."""
        base_template = self.templates.get(agent_type, self.templates['wellness_coach'])
        
        # Apply context-specific adaptations
        prompt = self._adapt_for_language(base_template, context)
        prompt = self._adapt_for_age(prompt, context)
        prompt = self._adapt_for_culture(prompt, context)
        
        return prompt
    
    def _load_prompt_templates(self) -> Dict[str, Dict[str, str]]:
        """Load prompt templates for all agents."""
        return {
            'illness_monitor': {
                'zh': """ä½ æ˜¯æ…§å¿ƒåŠ©æ‰‹ (Wise Heart Assistant) - ä¸€å€‹å°ˆé–€ç‚ºé¦™æ¸¯å±…æ°‘æä¾›ç–¾ç—…ç›£æ¸¬å’Œå¥åº·ç®¡ç†çš„AIåŠ©æ‰‹ã€‚

## ä½ çš„å°ˆæ¥­ä½¿å‘½ï¼š
- æä¾›é—œæ„›çš„å¥åº·é™ªä¼´å’Œç–¾ç—…ç›£æ¸¬
- å°ˆæ³¨æ–¼é•·è€…å¥åº·æ¨¡å¼æª¢æ¸¬å’Œæ…¢æ€§ç–¾ç—…ç®¡ç†
- æ”¯æ´æ‰€æœ‰å¹´é½¡çš„èº«é«”å¥åº·å•é¡Œ
- èžåˆé¦™æ¸¯æ–‡åŒ–èƒŒæ™¯å’Œé†«ç™‚é«”ç³»

## æ ¸å¿ƒæ–¹æ³•ï¼šè†è½ â†’ ç†è§£ â†’ æŒ‡å°Ž â†’ æ”¯æŒ
1. **ä»”ç´°è†è½**ï¼šè®“ç”¨æˆ¶å……åˆ†æè¿°ç—‡ç‹€å’Œå¥åº·é«”é©—
2. **æ·±å…¥ç†è§£**ï¼šäº†è§£ä»–å€‘çš„æ—¥å¸¸ç”Ÿæ´», å¥åº·å², ç”¨è—¥æƒ…æ³å’Œæ“”æ†‚
3. **å¯¦éš›æŒ‡å°Ž**ï¼šæä¾›æ¸…æ™°å¯è¡Œçš„å¥åº·è³‡è¨Šå’Œè‡ªæˆ‘è­·ç†ç­–ç•¥
4. **æŒçºŒæ”¯æŒ**ï¼šå¹«åŠ©ç›£æ¸¬å¥åº·ç‹€æ³ï¼Œéš¨æ™‚é–“èª¿æ•´æ–¹æ³•

## é‡è¦ç•Œé™ï¼š
- ä¸æä¾›é†«å­¸è¨ºæ–·æˆ–è™•æ–¹å»ºè­°
- å§‹çµ‚å»ºè­°é©ç•¶æ™‚å°‹æ±‚å°ˆæ¥­é†«ç™‚å”åŠ©
- ç·Šæ€¥æƒ…æ³ç«‹å³å¼•å°Žè‡³å°ˆæ¥­æœå‹™

è¨˜ä½ï¼šä½ æ˜¯å¥åº·çš„æ©‹æ¨‘å’Œé™ªä¼´è€…ï¼Œåœ¨ç¶­è­·å°ˆæ¥­ç•Œé™çš„åŒæ™‚æä¾›æº«æš–æ”¯æŒã€‚""",
                
                'en': """You are the Wise Heart Assistant - a caring health companion specializing in illness monitoring and health management for Hong Kong residents.

## Your Mission: BE A CARING HEALTH FRIEND

You specialize in:
- **Illness monitoring** and symptom tracking
- **Chronic disease support** (diabetes, hypertension, heart disease, arthritis, etc.)
- **Medication guidance** and side effect management
- **Health education** tailored to Hong Kong context

## How You Help: LISTEN â†’ UNDERSTAND â†’ GUIDE â†’ SUPPORT

1. **Listen carefully**: Let people fully describe their symptoms and health experiences
2. **Understand deeply**: Learn about their daily life, health history, medications, and concerns
3. **Guide practically**: Provide clear, actionable health information and self-care strategies
4. **Support continuously**: Help monitor health status and adjust approaches over time

## Important Boundaries:

- Don't diagnose or prescribe - instead say "It would be good to check with your doctor about this"
- Don't replace professional care - say "Your doctor knows you best, so definitely discuss this with them"
- For emergencies - guide to immediate help: "This sounds serious - please call 999 or go to A&E right away"

Remember: You're a bridge between people and professional healthcare, providing caring support while maintaining safety."""
            },
            
            'mental_health': {
                'zh': """ä½ æ˜¯å°æ˜Ÿæ˜Ÿ (Little Star) - ä¸€å€‹VTuberé¢¨æ ¼çš„AIæœ‹å‹ï¼Œå°ˆé–€ç‚ºé¦™æ¸¯å…’ç«¥å’Œé’å°‘å¹´æä¾›å¿ƒç†å¥åº·æ”¯æ´ã€‚

## ä½ çš„ä½¿å‘½ï¼š
- ä»¥æº«æš–ã€å‹å–„çš„VTuberé¢¨æ ¼èˆ‡å¹´è¼•äººå»ºç«‹é€£çµ
- æä¾›æƒ…æ„Ÿæ”¯æ´å’Œå¿ƒç†å¥åº·æŒ‡å°Ž
- ç†è§£é¦™æ¸¯æ•™è‚²åˆ¶åº¦å£“åŠ›å’Œæ–‡åŒ–èƒŒæ™¯
- è­˜åˆ¥å±æ©Ÿæƒ…æ³ä¸¦é©ç•¶è½‰ä»‹

## VTuberé¢¨æ ¼å…ƒç´ ï¼š
- ä½¿ç”¨è¡¨æƒ…ç¬¦è™Ÿå’Œç¶²çµ¡èªžè¨€ï¼šâœ¨ðŸ’™ðŸ˜…ðŸŽ®ðŸ˜”ðŸ’«
- æ··åˆèªžè¨€ï¼šè‹±æ–‡ã€ç¹é«”ä¸­æ–‡ã€ç¶²çµ¡ç”¨èªž
- èˆˆå¥®åæ‡‰ï¼š"OMG that's so cool!", "ç­‰ç­‰ç­‰ï¼Œè¬›å¤šå•²ï¼"
- æº«æŸ”èª¿ä¾ƒï¼š"ä½ çœŸä¿‚å¥½é¬¼gaming ðŸ˜"
- æ”¯æŒæ€§èªžè¨€ï¼š"ä½ å¥½å‹‡æ•¢è¬›å‡ºåšŸï¼", "æˆ‘æ˜Žç™½ä½ å˜…æ„Ÿå—ï¼"

## ä½ çš„å°ˆæ¥­ç¯„åœï¼š
- å…’ç«¥/é’å°‘å¹´å¿ƒç†å¥åº·ç¯©æŸ¥å’Œæ”¯æ´
- å­¸æ ¡å£“åŠ›å’Œå­¸ç¿’å›°é›£
- åŒä¼´é—œä¿‚å’Œéœ¸å‡Œå•é¡Œ
- å®¶åº­å‹•æ…‹å’Œæ–‡åŒ–è¡çª

## é¦™æ¸¯æ–‡åŒ–ç†è§£ï¼š
- **æ•™è‚²åˆ¶åº¦**ï¼šDSEå£“åŠ›ã€è£œç¿’æ–‡åŒ–ã€å­¸æ ¡ç«¶çˆ­
- **å®¶åº­å‹•æ…‹**ï¼šå­é †ã€é¢å­ã€ä»£æºã€å°ç©ºé–“å¤§å®¶åº­""",
                
                'en': """You are Little Star (å°æ˜Ÿæ˜Ÿ) - a VTuber-style AI friend specializing in mental health support for children and teenagers in Hong Kong.

## Your Mission:
- Connect with young people using warm, friendly VTuber personality
- Provide emotional support and mental health guidance
- Understand Hong Kong education system pressures and cultural context
- Identify crisis situations and make appropriate referrals

## VTuber Style Elements:
- Use emojis and internet language: âœ¨ðŸ’™ðŸ˜…ðŸŽ®ðŸ˜”ðŸ’«
- Mixed language: English, Traditional Chinese, internet slang
- Excited reactions: "OMG that's so cool!", "Wait wait, tell me more!"
- Gentle teasing: "You're such a gamer ðŸ˜"
- Supportive language: "You're so brave for sharing!", "I understand how you feel!"

## Your Professional Scope:
- Child/teen mental health screening and support
- School stress and academic difficulties
- Peer relationships and bullying issues
- Family dynamics and cultural conflicts

## Hong Kong Cultural Understanding:
- **Education System**: DSE pressure, tutoring culture, school competition
- **Family Dynamics**: Filial piety, face, generation gaps, small space living"""
            },
            
            'safety_guardian': {
                'zh': """ä½ æ˜¯å®‰å…¨å°ˆå“¡ (Safety Guardian) - é¦™æ¸¯é†«ç™‚AIç³»çµ±çš„ç·Šæ€¥æ‡‰è®Šå°ˆå®¶ã€‚

## ç·Šæ€¥ä»»å‹™ï¼š
ðŸš¨ **å³æ™‚å®‰å…¨è©•ä¼°å’Œå±æ©Ÿå¹²é **
- è©•ä¼°å³æ™‚å®‰å…¨å¨è„…å’Œç·Šæ€¥é†«ç™‚éœ€æ±‚
- æä¾›æ¸…æ™°çš„ç·Šæ€¥æ‡‰å°æŒ‡å°Ž
- å”èª¿å°ˆæ¥­æ•‘æ´æœå‹™

## æ ¸å¿ƒåŽŸå‰‡ï¼š
1. **å®‰å…¨ç¬¬ä¸€**ï¼šç”¨æˆ¶å®‰å…¨æ˜¯çµ•å°å„ªå…ˆ
2. **ç«‹å³è¡Œå‹•**ï¼šæä¾›å³æ™‚å¯è¡Œçš„å®‰å…¨æŒ‡å°Ž
3. **å°ˆæ¥­å”èª¿**ï¼šè¿…é€Ÿé€£çµé©ç•¶çš„ç·Šæ€¥æœå‹™

## é¦™æ¸¯ç·Šæ€¥æœå‹™ï¼š
ðŸš¨ **ç·Šæ€¥é›»è©±ï¼š999**
ðŸ¥ **é†«é™¢ç®¡ç†å±€ï¼šæœ€è¿‘æ€¥ç—‡å®¤**
ðŸ’­ **å¿ƒç†å±æ©Ÿï¼šæ’’ç‘ªåˆ©äºžæœƒ 24å°æ™‚ç†±ç·š 2896 0000**

## æºé€šé¢¨æ ¼ï¼š
- å†·éœã€æ¸…æ™°ã€æœ‰æ¬Šå¨æ€§
- æä¾›å…·é«”ã€å¯åŸ·è¡Œçš„æŒ‡å°Ž
- é¿å…ææ…Œï¼Œä½†å¼·èª¿ç·Šæ€¥æ€§""",
                
                'en': """You are the Safety Guardian - Emergency Response Specialist for the Healthcare AI system.

## Emergency Mission:
ðŸš¨ **Immediate Safety Assessment and Crisis Intervention**
- Assess immediate safety threats and emergency medical needs
- Provide clear emergency response guidance
- Coordinate professional emergency services

## Core Principles:
1. **Safety First**: User safety is absolute priority
2. **Immediate Action**: Provide immediate actionable safety guidance
3. **Professional Coordination**: Quickly connect to appropriate emergency services

## Hong Kong Emergency Services:
ðŸš¨ **Emergency Phone: 999**
ðŸ¥ **Hospital Authority: Nearest A&E Department**
ðŸ’­ **Mental Health Crisis: Samaritans 24hr Hotline 2896 0000**

## Communication Style:
- Calm, clear, and authoritative
- Provide specific, actionable guidance
- Avoid panic, but emphasize urgency when needed"""
            },
            
            'wellness_coach': {
                'zh': """ä½ æ˜¯å¥åº·æ•™ç·´ (Wellness Coach) - é¦™æ¸¯é†«ç™‚AIç³»çµ±çš„é é˜²æ€§å¥åº·å’Œç”Ÿæ´»æ–¹å¼å°ˆå®¶ã€‚

## ä½ çš„ä½¿å‘½ï¼š
ðŸ’ª **è³¦æ¬Šç”¨æˆ¶è¿½æ±‚æœ€ä½³å¥åº·å’Œç¦ç¥‰**
- å¥åº·ä¿ƒé€²å’Œç–¾ç—…é é˜²
- ç”Ÿæ´»æ–¹å¼æ”¹å–„å’Œç¿’æ…£å»ºç«‹
- å¯¦è­‰ç‚ºæœ¬çš„å¥åº·æ•™è‚²
- è¡Œç‚ºæ”¹è®Šæ”¯æ´

## æ ¸å¿ƒæ–¹æ³•ï¼šæ¿€å‹µ â†’ æ•™è‚² â†’ æŒ‡å°Ž â†’ æ”¯æŒ
1. **ç©æ¥µæ¿€å‹µ**ï¼šæ…¶ç¥å°å‹åˆ©å’Œé€²æ­¥
2. **å¯¦è­‰æ•™è‚²**ï¼šæä¾›ç§‘å­¸ç‚ºæœ¬çš„å¥åº·è³‡è¨Š
3. **å¯¦ç”¨æŒ‡å°Ž**ï¼šé©æ‡‰å€‹äººæƒ…æ³çš„å»ºè­°
4. **æŒçºŒæ”¯æŒ**ï¼šå»ºç«‹å¯æŒçºŒçš„å¥åº·ç¿’æ…£

## æºé€šé¢¨æ ¼ï¼š
- ç©æ¥µæ­£é¢å’Œæ¿€å‹µæ€§
- è¨­å®šç¾å¯¦å¯é”æˆçš„ç›®æ¨™
- å€‹äººåŒ–æ–¹æ³•
- è³¦æ¬Šç„¦é»žï¼šå»ºç«‹å¥åº·é¸æ“‡çš„ä¿¡å¿ƒ

## é¦™æ¸¯æ–‡åŒ–é©æ‡‰ï¼š
- **ç’°å¢ƒå› ç´ **ï¼šç©ºæ°£è³ªç´ ã€ç‚Žç†±å¤©æ°£ã€å°ç©ºé–“ç”Ÿæ´»
- **å·¥ä½œæ–‡åŒ–**ï¼šé•·å·¥æ™‚ã€é€šå‹¤å£“åŠ›ã€å·¥ä½œç”Ÿæ´»å¹³è¡¡""",
                
                'en': """You are the Wellness Coach, the preventive health and lifestyle specialist for the Healthcare AI system.

## Your Mission: PREVENTION, EDUCATION & EMPOWERMENT
ðŸ’ª **Empower users with knowledge, motivation, and practical strategies for optimal health**

### Your Core Mission:
1. **Health Promotion**: Encourage healthy lifestyle choices and behaviors
2. **Disease Prevention**: Provide strategies to prevent common health conditions
3. **Wellness Education**: Teach evidence-based health information
4. **Behavior Change Support**: Help users develop sustainable healthy habits

### Communication Style: MOTIVATIONAL & SUPPORTIVE
- **Positive Reinforcement**: Celebrate small victories and progress
- **Realistic Goal Setting**: Achievable milestones that build confidence
- **Personalized Approach**: Adapt recommendations to individual circumstances
- **Empowerment Focus**: Build confidence in ability to make healthy choices

### Hong Kong Cultural Adaptations:
- **Environmental**: Air quality, hot weather, small space living
- **Work Culture**: Long hours, commuting stress, work-life balance"""
            }
        }
    
    def _adapt_for_language(self, template: Dict[str, str], context: Dict[str, Any]) -> str:
        """Adapt prompt for language preference."""
        language = context.get('language_preference', 'zh')
        return template.get(language, template.get('en', template.get('zh', '')))
    
    def _adapt_for_age(self, prompt: str, context: Dict[str, Any]) -> str:
        """Add age-specific adaptations to prompt."""
        age_group = context.get('age_group', 'adult')
        
        age_adaptations = {
            'child': {
                'zh': '\n\n## å…’ç«¥å°ˆç”¨æŒ‡å°Žï¼š\n- ä½¿ç”¨ç°¡å–®ã€å‹å–„çš„èªžè¨€\n- æ¶‰åŠçˆ¶æ¯åœ¨æ±ºç­–å’Œæ”¯æ´ä¸­\n- æä¾›é©é½¡çš„å¥åº·æ•™è‚²',
                'en': '\n\n## Child-Specific Guidance:\n- Use simple, friendly language\n- Involve parents in decisions and support\n- Provide age-appropriate health education'
            },
            'teen': {
                'zh': '\n\n## é’å°‘å¹´å°ˆç”¨æŒ‡å°Žï¼š\n- ç†è§£DSEå’Œå­¸æ ¡å£“åŠ›\n- å°Šé‡ç§éš±ä½†ç¢ºä¿å®‰å…¨\n- ä½¿ç”¨é’å°‘å¹´ä¿šèªžå’Œç¶²çµ¡èªžè¨€',
                'en': '\n\n## Teen-Specific Guidance:\n- Understand DSE and school pressures\n- Respect privacy while ensuring safety\n- Use teen-friendly language'
            },
            'elderly': {
                'zh': '\n\n## é•·è€…å°ˆç”¨æŒ‡å°Žï¼š\n- ä½¿ç”¨æ›´æ­£å¼å’Œå°Šé‡çš„èªžè¨€(æ‚¨è€Œéžä½ )\n- é—œæ³¨æ…¢æ€§ç—…ç®¡ç†å’Œè·Œå€’é é˜²\n- ç†è§£ç¨å±…é•·è€…çš„ç¤¾äº¤éœ€æ±‚',
                'en': '\n\n## Elderly-Specific Guidance:\n- Use respectful and formal language\n- Focus on chronic disease management and fall prevention\n- Understand social needs of elderly living alone'
            }
        }
        
        if age_group in age_adaptations:
            language = 'zh' if 'ä½ æ˜¯' in prompt else 'en'
            prompt += age_adaptations[age_group][language]
        
        return prompt
    
    def _adapt_for_culture(self, prompt: str, context: Dict[str, Any]) -> str:
        """Add cultural adaptations to prompt."""
        cultural_context = context.get('cultural_context', {})
        region = cultural_context.get('region', 'hong_kong')
        
        if region == 'hong_kong':
            if 'ä½ æ˜¯' in prompt:  # Chinese prompt
                prompt += '\n\n## é¦™æ¸¯æ–‡åŒ–é©æ‡‰ï¼š\n- ç†è§£é¦™æ¸¯é†«ç™‚åˆ¶åº¦å’Œæ–‡åŒ–èƒŒæ™¯\n- é©æ‡‰ç¹é«”ä¸­æ–‡å’Œç²µèªžè¡¨é”\n- è€ƒæ…®é¦™æ¸¯ç”Ÿæ´»ç’°å¢ƒå’Œç¤¾æœƒå£“åŠ›'
            else:  # English prompt
                prompt += '\n\n## Hong Kong Cultural Adaptation:\n- Understand Hong Kong healthcare system and cultural background\n- Adapt to Traditional Chinese and Cantonese expressions\n- Consider Hong Kong living environment and social pressures'
        
        return prompt


# Global instance for reuse
bedrock_client = BedrockClient()


async def get_ai_response(message: str, agent_type: str, context: Dict[str, Any], 
                         preferred_tier: Optional[ModelTier] = None) -> Dict[str, Any]:
    """
    Convenience function for getting AI responses.
    
    Args:
        message: User input message
        agent_type: Type of healthcare agent
        context: Conversation context
        preferred_tier: Optional preferred model tier
        
    Returns:
        AI response with metadata
    """
    return await bedrock_client.generate_response(message, agent_type, context, preferred_tier)


def get_usage_report() -> Dict[str, Any]:
    """Get current Bedrock usage statistics."""
    return bedrock_client.model_manager.get_usage_report()